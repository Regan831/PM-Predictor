{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from datetime import datetime, timedelta\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "# import xgboost as xgb\n",
    "\n",
    "import constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_static_data():\n",
    "    static_sensor_data=[]\n",
    "    for sensor in constants.SENSOR_IDS:\n",
    "        list_ = []\n",
    "        for date in constants.SELECT_DATES:\n",
    "            filename = \"/Users/ryanegan/Documents/diss/RawData/static/{}/{}.csv\".format(date, sensor)\n",
    "            df = pd.read_csv(filename,index_col=None)\n",
    "            df['lat'] = constants.STATIC_COORDS[sensor][0]\n",
    "            df['long'] = constants.STATIC_COORDS[sensor][1]\n",
    "            df['mobile'] = False\n",
    "            # Calibration\n",
    "            df['PM1'] = df['PM1'].values / constants.CALIBRATION_FACTORS[sensor][0]\n",
    "            df['PM2.5'] = df['PM2.5'].values / constants.CALIBRATION_FACTORS[sensor][1]\n",
    "            df['PM10'] = df['PM10'].values / constants.CALIBRATION_FACTORS[sensor][2]\n",
    "            df['temperature'] = df['temperature'].values / constants.CALIBRATION_FACTORS[sensor][3]\n",
    "            df['humidity'] = df['humidity'].values / constants.CALIBRATION_FACTORS[sensor][4]\n",
    "            # -----------\n",
    "            list_.append(df)\n",
    "        static_sensor_data.append(pd.concat(list_, axis = 0, ignore_index = False))\n",
    "    all_static_data = pd.concat(static_sensor_data, axis = 0, ignore_index = False)\n",
    "    # Select columns\n",
    "    all_static_data = all_static_data[constants.COLUMNS]\n",
    "    all_static_data = all_static_data.dropna()\n",
    "    return all_static_data\n",
    "\n",
    "def load_mobile_sensors():\n",
    "    list_ = []\n",
    "    for sensor in constants.MOBILE_SENSORS:\n",
    "        for date in constants.SELECT_DATES:\n",
    "            filename = \"/Users/ryanegan/Documents/diss/RawData/personal/{}/{}_{}.csv\".format(date, sensor, date)\n",
    "            df = pd.read_csv(filename,index_col=None)\n",
    "            list_.append(df)\n",
    "        mobile_sensor_data = (pd.concat(list_, axis = 0, ignore_index = False))\n",
    "        mobile_sensor_data['lat'] = mobile_sensor_data['latitude']\n",
    "        mobile_sensor_data['long'] = mobile_sensor_data['longitude']\n",
    "        mobile_sensor_data['mobile'] = True\n",
    "        # Calibration\n",
    "        df['PM1'] = df['PM1'].values / constants.CALIBRATION_FACTORS[sensor][0]\n",
    "        df['PM2.5'] = df['PM2.5'].values / constants.CALIBRATION_FACTORS[sensor][1]\n",
    "        df['PM10'] = df['PM10'].values / constants.CALIBRATION_FACTORS[sensor][2]\n",
    "        df['temperature'] = df['temperature'].values / constants.CALIBRATION_FACTORS[sensor][3]\n",
    "        df['humidity'] = df['humidity'].values / constants.CALIBRATION_FACTORS[sensor][4]\n",
    "        #Select columns\n",
    "        mobile_sensor_data = mobile_sensor_data[constants.COLUMNS]\n",
    "    return mobile_sensor_data\n",
    "\n",
    "#Transform to grid coordinates\n",
    "def grid_lat_coord(lat):\n",
    "    if lat < 55.93814 or lat > 55.94686:\n",
    "        return -1\n",
    "    i = 0\n",
    "    for lat_check in np.linspace(55.93814,55.94686,constants.GRID_SIZE+1)[1:]:\n",
    "        if lat < lat_check:\n",
    "            return 19-i\n",
    "        i+=1\n",
    "\n",
    "def grid_long_coord(long):\n",
    "    if long < -3.19665 or long > -3.18123:\n",
    "        return -1\n",
    "    i = 0\n",
    "    for long_check in np.linspace(-3.19665,-3.18123,constants.GRID_SIZE+1)[1:]:\n",
    "        if long < long_check:\n",
    "            return i\n",
    "        i+=1\n",
    "\n",
    "def transform_to_grid_coordinates(data):\n",
    "    data['lat_grid'] = data['lat'].apply(grid_lat_coord)\n",
    "    data['long_grid'] = data['long'].apply(grid_long_coord)\n",
    "    data = data[data['lat_grid'] >= 0]\n",
    "    data = data[data['long_grid'] >= 0]\n",
    "    return data\n",
    "\n",
    "def setIds(data):\n",
    "    startTime = constants.START_TIME\n",
    "    endTime = (datetime.strptime(startTime, '%Y-%m-%d %H:%M:%S') + timedelta(minutes=5)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    i = 0\n",
    "    j=0\n",
    "    data['timestep'] = ''\n",
    "    data['timestepContinuous'] = ''\n",
    "    \n",
    "    while startTime < constants.END_TIME:\n",
    "        \n",
    "        data['timestep'] = np.where((data['Timestamp'] >= startTime) & (data['Timestamp'] < endTime), int(i), data['timestep'])\n",
    "        i+=1\n",
    "        \n",
    "        if (data[(data['Timestamp'] >= startTime) & (data['Timestamp'] < endTime)].shape[0] != 0):\n",
    "            data['timestepContinuous'] = np.where((data['Timestamp'] >= startTime) & (data['Timestamp'] < endTime), int(j), data['timestepContinuous'])\n",
    "            j+=1\n",
    "        startTime = endTime\n",
    "        endTime = (datetime.strptime(endTime, '%Y-%m-%d %H:%M:%S') + timedelta(minutes=constants.WINDOW)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# staticData = load_static_data()\n",
    "# staticData = transform_to_grid_coordinates(staticData)\n",
    "# mobileData = load_mobile_sensors()\n",
    "# mobileData = transform_to_grid_coordinates(mobileData)\n",
    "# allData = pd.concat([staticData, mobileData], ignore_index=True)\n",
    "# setIds(allData)\n",
    "# allData\n",
    "\n",
    "\n",
    "with open(\"all_data_15_min.txt\", \"rb\") as fp:   # Unpickling\n",
    "    all_data = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONLY RUN ONCE AT BEGINNING\n",
    "# label_grid = realValueGrid(allData)\n",
    "# with open(\"grid_labels_15.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(realValueGrid, fp)\n",
    "\n",
    "#Get true values from existing file\n",
    "with open(\"grid_labels.txt\", \"rb\") as fp:   # Unpickling\n",
    "    grid_labels = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X, y):\n",
    "    class PrintDot(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            if epoch % 100 == 0: print('')\n",
    "            print('.', end='')\n",
    "            \n",
    "    early_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n",
    "    n_timesteps, n_features, n_outputs = X.shape[1], X.shape[2], y.shape[1]\n",
    "    # define model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(layers.Dense(100, activation='relu'))\n",
    "    model.add(layers.Dense(n_outputs))\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    \n",
    "    # fit network\n",
    "    model.fit(X, y, epochs=1000, batch_size=16, verbose=0, callbacks=[early_stop, PrintDot()])\n",
    "    return model\n",
    "\n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(data, labels, n_input, n_out=4):\n",
    "    # flatten data\n",
    "#     data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            x_input = data[in_start:in_end,:]\n",
    "#             x_input = x_input.reshape((len(x_input), 1))\n",
    "            X.append(x_input)\n",
    "            y.append(labels[in_end:out_end])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_with_models(grid_size):\n",
    "    grid = [[] for i in range(grid_size)]\n",
    "    for i, line in enumerate(grid):\n",
    "        for j in range(20):\n",
    "            staticCoords = list(constants.STATIC_COORDS_GRID.values())\n",
    "            if ([i,j] in staticCoords):\n",
    "                line.append(createStationaryModel())\n",
    "            else:\n",
    "                line.append(PassiveAggressiveRegressor(C=1, epsilon=0.1, loss='epsilon_insensitive', max_iter=100, random_state=0,tol=1e-3))\n",
    "    return grid\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create grid with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allData = allData.assign(nextPM=allData['PM2.5'].shift(-3))\n",
    "# allData['hour'] = allData.apply(lambda row: datetime.strptime(row.Timestamp[:19], '%Y-%m-%d %H:%M:%S').hour, axis=1)\n",
    "# allData['grid'] = allData.apply(lambda row: str([row.lat_grid, row.long_grid]), axis=1)\n",
    "\n",
    "# train = allData[allData['timestepContinuous'] < 1450]\n",
    "# valid = allData[allData['timestepContinuous'] >= 1450]\n",
    "\n",
    "# staticData['hour'] = staticData.apply(lambda row: datetime.strptime(row.Timestamp[:19], '%Y-%m-%d %H:%M:%S').hour, axis=1)\n",
    "# mobileData['hour'] = mobileData.apply(lambda row: datetime.strptime(row.Timestamp[:19], '%Y-%m-%d %H:%M:%S').hour, axis=1)\n",
    "\n",
    "# setIds(staticData)\n",
    "# setIds(mobileData)\n",
    "\n",
    "train = all_data[all_data['timestepContinuous']  < 1450]\n",
    "valid = all_data[all_data['timestepContinuous']  >= 1450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a forecast\n",
    "def forecast(model, history, n_input):\n",
    "    # flatten data\n",
    "    data = np.array(history)\n",
    "    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "    # retrieve last observations for input data\n",
    "    input_x = data[-n_input:, :]\n",
    "    # reshape into [1, n_input, features]\n",
    "    input_x = input_x.reshape((1, len(input_x), len(constants.LSTM_TRAINING_FEATURES)))\n",
    "    # forecast the next week\n",
    "    \n",
    "    yhat = model.predict(input_x, verbose=0)\n",
    "    # we only want the vector forecast\n",
    "    yhat = yhat[0]\n",
    "    return yhat\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(train, valid, valid_labels, n_input, model):\n",
    "\n",
    "    history = [x for x in train]\n",
    "    # walk-forward validation over each week\n",
    "    predictions = list()\n",
    "    for i in range(len(valid)):\n",
    "        # predict the week\n",
    "        yhat_sequence = forecast(model, history, n_input)\n",
    "        # store the predictions\n",
    "        predictions.append(yhat_sequence)\n",
    "\n",
    "        # get real observation and add to history for predicting the next week\n",
    "        history.append(valid[i, :])\n",
    "    # evaluate predictions days for each week\n",
    "    predictions = np.array(predictions)\n",
    "    score, scores = evaluate_forecasts(valid_labels[:, :], predictions)\n",
    "    return score, scores\n",
    "\n",
    "# evaluate one or more weekly forecasts against expected values\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    scores = list()\n",
    "    # calculate an RMSE score for each day\n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_absolute_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "#         rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(mse)\n",
    "    # calculate overall RMSE\n",
    "    s = 0\n",
    "    for row in range(actual.shape[0]):\n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    score = math.sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".................................(0.3899379714595772, [0.1821388372924833, 0.19462529926853553, 0.21756784608536717, 0.2196212384592996])\n",
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "......................................................................................(0.7008231652643669, [0.23735412165890057, 0.3625715731644274, 0.39042606035179306, 0.4081763290958346])\n",
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      ".........(0.33922134956917066, [0.14124228300730934, 0.16726727907228822, 0.21132159741985357, 0.22233261321208247])\n",
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................(0.3566663077376147, [0.10397069838763869, 0.13327834669463987, 0.1620805927609981, 0.18263277752329607])\n",
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................(0.39867720405114865, [0.2163460952096453, 0.252139473899109, 0.27471895924482337, 0.2834165657624])\n",
      "\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "....................................................................................................\n",
      "...........................................(0.2032355576875446, [0.12367445708043481, 0.13364657153672632, 0.15593182447581366, 0.1626694447974113])\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = list(), list()\n",
    "for coords in constants.STATIC_COORDS_GRID.values():\n",
    "    x = coords[0]\n",
    "    y = coords[1]\n",
    "    train_data = train[(train['lat_grid'] == x) & (train['long_grid'] == y)]\n",
    "    valid_data = valid[(valid['lat_grid'] == x) & (valid['long_grid'] == y)]\n",
    "    train_labels = train_data['PM2.5']\n",
    "    valid_labels = valid_data['PM2.5']\n",
    "    train_data = train_data[constants.LSTM_TRAINING_FEATURES].to_numpy()\n",
    "    valid_data = valid_data[constants.LSTM_TRAINING_FEATURES].to_numpy()\n",
    "    \n",
    "    x_train, y_train = to_supervised(train_data, np.array(train_labels), 4, 4)\n",
    "    x_valid, y_valid = to_supervised(valid_data, np.array(valid_labels), 4, 4)\n",
    "\n",
    "#     inputs.append(X)\n",
    "#     outputs.append(labels)\n",
    "    model = build_model(x_train, y_train)\n",
    "    print(evaluate_model(x_train, x_valid, y_valid, 4, model))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateModels(data, grid, labels):\n",
    "    for i in range(len(grid)):\n",
    "        for j in range(len(grid[0])):\n",
    "            staticCoords = list(constants.STATIC_COORDS_GRID.values())\n",
    "#             print(data)\n",
    "            X = data[(data['lat_grid'] == i) & (data['long_grid'] == j)]\n",
    "#             print(X)\n",
    "#             X = X.mean()\n",
    "#             print(len(labels))\n",
    "#             print(len(labels[0]))\n",
    "            label = labels[0][i][j]\n",
    "#             X = X[constants.TRAINING_FEATURES]\n",
    "            \n",
    "            #Put data in proper format\n",
    "#             label = np.array([label])\n",
    "            \n",
    "            if ([i,j] in staticCoords and label != None and not X.empty):\n",
    "                X = X[constants.TRAINING_FEATURES]\n",
    "                label = np.array([label])\n",
    "                online_model = grid[i][j][0]\n",
    "                online_model.fit(\n",
    "                    X, label, batch_size=1,\n",
    "                    epochs=10, verbose=0)\n",
    "                \n",
    "                grid[i][j][2].partial_fit(X, label)\n",
    "            \n",
    "#             else:\n",
    "#                 grid[i][j].partial_fit(X, labels)\n",
    "                \n",
    "def trainNNs(trainData, grid):\n",
    "    \n",
    "    class PrintDot(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            if epoch % 100 == 0: print('')\n",
    "            print('.', end='')\n",
    "                \n",
    "    for coord in constants.STATIC_COORDS_GRID.values():\n",
    "        labels = []\n",
    "        X = pd.DataFrame(columns = constants.TRAINING_FEATURES)\n",
    "        start = int(trainData['timestepContinuous'].min())\n",
    "        end = int(trainData['timestepContinuous'].max())\n",
    "        for i in range(start + 1, end + 1):\n",
    "            curInput = trainData[(trainData['lat_grid'] == coord[0]) & (trainData['long_grid'] == coord[1]) & (trainData['timestepContinuous'] == i-1)]\n",
    "            if((not curInput.empty) and (grid_labels[i][coord[0]][coord[1]] != None)):\n",
    "                labels.append(grid_labels[i][coord[0]][coord[1]])\n",
    "                curInput = curInput[constants.TRAINING_FEATURES]\n",
    "#                 X.loc[len(X)] = curInput\n",
    "                X = pd.concat([X, curInput])\n",
    "        \n",
    "        allData1 = trainData[(trainData['lat_grid'] == coord[0]) & (trainData['long_grid'] == coord[1]) & (trainData['timestepContinuous'] < end)]\n",
    "        print(allData1)\n",
    "        print(labels)\n",
    "#         USE GRID\n",
    "#         train_labels = allData1.pop('nextPM')\n",
    "#         X = allData1[constants.TRAINING_FEATURES]\n",
    "        online_model = grid[coord[0]][coord[1]][0]\n",
    "        batch_model = grid[coord[0]][coord[1]][1]\n",
    "        par_model = grid[coord[0]][coord[1]][2]\n",
    "        \n",
    "        early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "        \n",
    "        par_model.fit(X, labels)\n",
    "        \n",
    "        online_model.fit(\n",
    "            X, labels, batch_size=16,\n",
    "            epochs=1000, verbose=0, validation_split = .2, callbacks=[ PrintDot()])\n",
    "\n",
    "        history = batch_model.fit(\n",
    "            X, labels, batch_size=16,\n",
    "            epochs=1000, validation_split = .2, verbose=0, callbacks=[ PrintDot()])\n",
    "        \n",
    "        plot_history(history)\n",
    "        \n",
    "\n",
    "        \n",
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "           label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "           label = 'Val Error')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainNNs(train, model_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realValueGrid(data):\n",
    "    values = []\n",
    "    for i in range(int(data['timestepContinuous'].max())):\n",
    "        print(i)\n",
    "        value_matrix = [[[] for x in range(constants.GRID_SIZE)] for y in range(constants.GRID_SIZE)]\n",
    "        for x in range(constants.GRID_SIZE):\n",
    "            for y in range(constants.GRID_SIZE):\n",
    "                curData = data[(data['lat_grid'] == x) & (data['long_grid'] == y) & (data['timestepContinuous'] == i)]\n",
    "                if(not curData.empty):\n",
    "                    curData = curData.mean()\n",
    "                    value_matrix[x][y] = curData['PM2.5']\n",
    "                else:\n",
    "                    value_matrix[x][y] = None\n",
    "        values.append(value_matrix)\n",
    "    return values\n",
    "\n",
    "def predictOneTimestep(data, grid, timestep):\n",
    "    pred_matrix = [[[] for x in range(len(grid))] for y in range(len(grid[0]))]\n",
    "    label_matrix = [[[] for x in range(len(grid))] for y in range(len(grid[0]))]\n",
    "    for i in range(len(grid)):\n",
    "        for j in range(len(grid[0])):\n",
    "            X = data[(data['lat_grid'] == i) & (data['long_grid'] == j)]\n",
    "#             X = X.mean()\n",
    "#             label = X.pop('nextPM')\n",
    "            X = X[constants.TRAINING_FEATURES]\n",
    "            \n",
    "#             if(not X.empty):\n",
    "#                 baseline = list(X[(X['lat_grid'] == coord[0]) & (X['long_grid'] == coord[1])]['PM2.5'])[0]\n",
    "\n",
    "            label = grid_labels[timestep + 1][i][j]\n",
    "            if([i,j] in constants.STATIC_COORDS_GRID.values() and not label == None and not X.empty):\n",
    "                online_model = grid[i][j][0]\n",
    "                batch_model = grid[i][j][1]\n",
    "                par_model = grid[i][j][2]\n",
    "#                 baseline = list(X[(X['lat_grid'] == coord[0]) & (X['long_grid'] == coord[1])]['PM2.5'])[0]\n",
    "                \n",
    "#                 baseline = X['PM2.5'].iloc[0]\n",
    "                X = np.array(X)\n",
    "                                \n",
    "                online_pred = online_model.predict(X)\n",
    "                batch_pred = batch_model.predict(X)\n",
    "                par_pred = par_model.predict(X)\n",
    "#                 pred_matrix[i][j] = (online_pred[0][0], batch_pred[0][0], label, par_pred[0], baseline)\n",
    "\n",
    "                pred_matrix[i][j] = (online_pred[0][0], batch_pred[0][0], label, par_pred[0])\n",
    "#             else:\n",
    "#                 #do par predict\n",
    "                \n",
    "    online_pred_matrix = interpolate(pred_matrix, 0)\n",
    "    batch_pred_matrix = interpolate(pred_matrix, 1)\n",
    "    par_pred_matrix = interpolate(pred_matrix, 3)\n",
    "    label_matrix = grid_labels[timestep + 1]\n",
    "    return (online_pred_matrix, batch_pred_matrix, par_pred_matrix, label_matrix)\n",
    "\n",
    "def interpolate(pred_matrix, pred_type):\n",
    "\n",
    "    #PLACE CNN HERE\n",
    "    data = np.array([[coord[0], coord[1], pred_matrix[coord[0]][coord[1]][pred_type]] for coord in constants.STATIC_COORDS_GRID.values() if len(pred_matrix[coord[0]][coord[1]]) > 0])\n",
    "    ok = OrdinaryKriging(data[:,0], data[:,1], data[:,2], variogram_model='gaussian', verbose=False, enable_plotting=False)\n",
    "    gridx = np.arange(0.0, constants.GRID_SIZE, 1)\n",
    "    gridy = np.arange(0.0, constants.GRID_SIZE, 1)\n",
    "    z, ss = ok.execute('grid', gridx, gridy)\n",
    "    return z\n",
    "    \n",
    "\n",
    "def getFullPredictions(grid, data):\n",
    "    minValue = int(data['timestepContinuous'].min())\n",
    "    maxValue = int(data['timestepContinuous'].max())\n",
    "    online_predictions = []\n",
    "    batch_predictions = []\n",
    "    ensemble = []\n",
    "    ensemble2 = []\n",
    "    ensemble3 = []\n",
    "    par_predictions = []\n",
    "    baseline_predictions = []\n",
    "    all_labels = []\n",
    "#     online_predictions = [[[] for x in range(len(grid))] for y in range(len(grid[0]))]\n",
    "#     batch_predictions = [[[] for x in range(len(grid))] for y in range(len(grid[0]))]\n",
    "#     ensemble = [[[] for x in range(len(grid))] for y in range(len(grid[0]))]\n",
    "#     ensemble2 = [[[] for x in range(len(grid))] for y in range(len(grid[0]))]\n",
    "#     ensemble3 = [[[] for x in range(len(grid))] for y in range(len(grid[0]))]\n",
    "#     par_predictions = [[[] for x in range(len(grid))] for y in range(len(grid[0]))]\n",
    "#     baseline_predictions = [[[] for x in range(len(grid))] for y in range(len(grid[0]))]\n",
    "    \n",
    "#     labels = [[[] for x in range(len(grid))] for y in range(len(grid[0]))]\n",
    "   \n",
    "    for timestep in range(minValue, maxValue):\n",
    "#     for timestep in range(1522, 1523):\n",
    "        print(timestep)\n",
    "        newData = data[(data['timestepContinuous'] == timestep)]\n",
    "        newData = newData[constants.FEATURES].dropna()\n",
    "        \n",
    "        online_pred_matrix, batch_pred_matrix, par_pred_matrix, label_matrix = predictOneTimestep(newData, model_grid, timestep)\n",
    "        online_predictions.append(online_pred_matrix)\n",
    "        batch_predictions.append(batch_pred_matrix)\n",
    "        par_predictions.append(par_pred_matrix)\n",
    "        \n",
    "        ensemble_pred = (.5 * online_pred_matrix) + (.5 * par_pred_matrix)\n",
    "        ensemble_pred2 = (.5 * par_pred_matrix) + (.5 * batch_pred_matrix)\n",
    "        ensemble_pred3 = (.5 * online_pred_matrix) + (.5 * batch_pred_matrix)\n",
    "        \n",
    "        ensemble.append(ensemble_pred)\n",
    "        ensemble2.append(ensemble_pred2)\n",
    "        ensemble3.append(ensemble_pred3)\n",
    "        \n",
    "        all_labels.append(label_matrix)\n",
    "        \n",
    "        updateModels(newData, model_grid, all_labels)\n",
    "    \n",
    "    return (online_predictions, batch_predictions, ensemble, ensemble2, ensemble3, all_labels, par_predictions, baseline_predictions)\n",
    "\n",
    "def getValidationError(preds, data):\n",
    "    minValue = int(data['timestepContinuous'].min())\n",
    "    maxValue = int(data['timestepContinuous'].max())\n",
    "#     values =  [[3, 6], [16, 6], [8, 5], [13, 13]', '[3, 10]', '[8, 13]']\n",
    "    labels_to_eval = []\n",
    "    preds_to_eval = []\n",
    "\n",
    "\n",
    "    for i, timestep in enumerate(range(minValue, maxValue)):\n",
    "        testData = data[(data['timestepContinuous'] == timestep)]\n",
    "        if(not testData.empty):\n",
    "            for x in range(constants.GRID_SIZE):\n",
    "                for y in range(constants.GRID_SIZE):\n",
    "                    if(timestep + 1 < maxValue and (not grid_labels[timestep + 1][x][y] is None)):\n",
    "                        if([x,y] not in constants.STATIC_COORDS_GRID.values()):\n",
    "                            labels_to_eval.append(grid_labels[timestep + 1][x][y])\n",
    "                            preds_to_eval.append(preds[i][x][y])\n",
    "\n",
    "#     plot_preds(preds_to_eval, labels_to_eval)\n",
    "    print(mean_absolute_error(preds_to_eval, labels_to_eval))\n",
    "    return mean_absolute_error(preds_to_eval, labels_to_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_preds, batch_preds, ensemble_preds, ensemble_preds2, ensemble_preds3, labels, par_preds, baseline_preds = getFullPredictions(model_grid, valid)\n",
    "onlineError = getValidationError(online_preds, valid)\n",
    "for i in range(10):\n",
    "    print(mean_absolute_error(baseline_preds[i], labels[i]))\n",
    "    print(mean_absolute_error(online_preds[coord[0]][coord[1]], labels[coord[0]][coord[1]]))\n",
    "    print(mean_absolute_error(batch_preds[coord[0]][coord[1]], labels[coord[0]][coord[1]]))\n",
    "    print(mean_absolute_error(ensemble_preds[coord[0]][coord[1]], labels[coord[0]][coord[1]]))\n",
    "    print(mean_absolute_error(ensemble_preds2[coord[0]][coord[1]], labels[coord[0]][coord[1]]))\n",
    "    print(mean_absolute_error(ensemble_preds3[coord[0]][coord[1]], labels[coord[0]][coord[1]]))\n",
    "    print(mean_absolute_error(par_preds[coord[0]][coord[1]], labels[coord[0]][coord[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlineError = getValidationError(online_preds, valid)\n",
    "batchError = getValidationError(batch_preds, valid)\n",
    "# onlineError = getValidationError(baseline_preds, labels, valid)\n",
    "parError = getValidationError(par_preds, valid)\n",
    "onlineError = getValidationError(ensemble_preds, valid)\n",
    "onlineError = getValidationError(ensemble_preds2, valid)\n",
    "onlineError = getValidationError(ensemble_preds3, valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_preds(preds, labels):\n",
    "#     hist = pd.DataFrame(history.history)\n",
    "#     hist['epoch'] = history.epoch\n",
    "\n",
    "    x = range(len(preds))\n",
    "    plt.figure()\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('PM 2.5')\n",
    "    plt.plot(x, preds,\n",
    "           label='Predictions')\n",
    "    plt.plot(x, labels,\n",
    "           label = 'True Value')\n",
    "    plt.ylim([0,5])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_online_preds = []\n",
    "all_batch_preds = []\n",
    "all_par_preds = []\n",
    "all_ensemble_preds = []\n",
    "all_ensemble_preds2 = []\n",
    "all_ensemble_preds3 = []\n",
    "all_labels = []\n",
    "all_baseline = []\n",
    "for coord in constants.STATIC_COORDS_GRID.values():\n",
    "    all_baseline = all_baseline + baseline_preds[coord[0]][coord[1]]\n",
    "    all_online_preds = all_online_preds + online_preds[coord[0]][coord[1]]\n",
    "    all_batch_preds = all_batch_preds + batch_preds[coord[0]][coord[1]]\n",
    "    all_par_preds = all_par_preds + par_preds[coord[0]][coord[1]]\n",
    "    all_ensemble_preds = all_ensemble_preds + ensemble_preds[coord[0]][coord[1]]\n",
    "    all_ensemble_preds2 = all_ensemble_preds2 + ensemble_preds2[coord[0]][coord[1]]\n",
    "    all_ensemble_preds3 = all_ensemble_preds3 + ensemble_preds3[coord[0]][coord[1]]\n",
    "    all_labels = all_labels + labels[coord[0]][coord[1]]  \n",
    "                                 \n",
    "print(mean_absolute_error(all_baseline, all_labels))\n",
    "print(mean_absolute_error(all_online_preds, all_labels))\n",
    "print(mean_absolute_error(all_batch_preds, all_labels))\n",
    "print(mean_absolute_error(all_par_preds, all_labels))\n",
    "print(mean_absolute_error(all_ensemble_preds, all_labels))\n",
    "print(mean_absolute_error(all_ensemble_preds2, all_labels))\n",
    "print(mean_absolute_error(all_ensemble_preds3, all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for timestep in range(4000, 4000 + len(valid['timestepContinuous'].unique())):\n",
    "for timestep in range(4000, 4000 + 5):\n",
    "\n",
    "    \n",
    "    #to_insert IS NEW DATA AT GIVEN TIME\n",
    "    to_predict = valid[valid['timestepContinuous'] == timestep]\n",
    "#     if mobile_sensor_data is not None:\n",
    "#         to_insert = to_insert.append( mobile_sensor_data[(mobile_sensor_data['Timestamp'] > start_window) & (mobile_sensor_data['Timestamp'] < end_window)])\n",
    "#     to_insert = to_insert.groupby(['lat_grid','long_grid']).mean()\n",
    "#     to_insert.reset_index(level=to_insert.index.names, inplace=True)\n",
    "    z = [[0 for j in range(constants.GRID_SIZE)] for i in range(constants.GRID_SIZE)]\n",
    "\n",
    "    #PLACE CNN HERE\n",
    "    ok = OrdinaryKriging(to_predict['long_grid'], to_predict['lat_grid'], to_predict['PM2.5'], variogram_model='gaussian', verbose=False, enable_plotting=False)\n",
    "    gridx = np.arange(0.0, 20, 1)\n",
    "    gridy = np.arange(0.0, 20, 1)\n",
    "    z, ss = ok.execute('grid', gridx, gridy)\n",
    "    # Substituto do Kriging so com os valores lidos\n",
    "    # Com ambos a funcionar, isto faz a correção dos erros do Kriging presente\n",
    "#     for index, row in to_predict.iterrows():\n",
    "#         z[int(row['lat_grid'])][int(row['long_grid'])] = row['PM2.5']\n",
    "#         hour[int(row['lat_grid'])][int(row['long_grid'])] = row['hour']\n",
    "    \n",
    "    updateModels(model_grid, timestep, valid)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_labels[1222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
